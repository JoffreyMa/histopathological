{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ed7e223-fd27-4de1-837a-58d6b648f8ca",
   "metadata": {},
   "source": [
    "# Emancipate from Hugging Face and its limits\n",
    "\n",
    "\n",
    "This implementation of Swin is in the transformers package by huggingface.\\\n",
    "Originally it is developped by Microsoft.\\\n",
    "\n",
    "In the transformers library, the model is declared here \n",
    "https://github.com/huggingface/transformers/blob/v4.26.1/src/transformers/models/swin/modeling_swin.py#L1152\n",
    "and is a child of SwinPreTrainedModel.\\\n",
    "At the end of initialization it calls PreTrainedModel.post_init method (SwinPreTrainedModel inherits from PreTrainedModel).\\\n",
    "The method itself calls self.init_weights() and itself the self._init_weights() from the child class\n",
    "\n",
    "\n",
    "What's more I can do data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a91fbc85-f0d2-4b8a-ab58-92c447880982",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from transformers import SwinConfig, SwinForImageClassification, ViTImageProcessor, Trainer, TrainingArguments\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, WeightedRandomSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import gc\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification, DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77a47212-b4e5-4f30-b724-a57e064eaa76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\") # in case gpu does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1211e5cb-67c3-4e3f-bb6e-84dfa4e7ceef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"\" # in case gpu does not work and the above does not work either"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea1339-6224-47bb-a642-2b4635f16d3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "## Config\n",
    "\n",
    "Import config of swin alread trained on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d1de26-e323-4835-b992-721526070a11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Shadow\\\\Documents\\\\Projets\\\\MastereIA\\\\DataChallenge\\\\histopathological\\\\exploration\\\\..'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "home = os.path.join(os.getcwd(), \"..\")\n",
    "home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1c0f0b-1259-48ae-b61e-6990b93c896c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "config_json = os.path.join(home, \"data\", \"model\", \"config\", \"config.json\")\n",
    "preprocessor_config_json = os.path.join(home, \"data\", \"model\", \"config\", \"preprocessor_config.json\")\n",
    "input_train = os.path.join(home, \"data\", \"input\", \"Train\")\n",
    "model_path = os.path.join(home, \"data\", \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d40b1d-46af-40a5-8688-6a1dd5f9aacd",
   "metadata": {},
   "source": [
    "I get a pre-trained model from Microsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e938d454-b898-49a0-b09d-f5ef3ed6616d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# possibilities from most likely to less\n",
    "# microsoft/swin-base-patch4-window7-224\n",
    "# microsoft/swin-base-patch4-window7-224-in22k\n",
    "pretrained_model = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n",
    "for param in pretrained_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a62f9a-4a77-4fe9-a6a7-92764edba4bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#pretrained_model.swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b839f989-adf9-4c65-a8f8-8c38b774f815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(config_json, 'r') as f:\n",
    "  swin_config_args = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bc1b428-90ec-4144-aa10-10e83a5bbc94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "configuration = SwinConfig(**swin_config_args)\n",
    "#configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71e31a6f-54af-400f-9db5-5f555ec6fba7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SwinForImageClassification(configuration)\n",
    "#model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c4ea886-c37d-4418-8322-178ae8a5d2d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.swin = pretrained_model.swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "477d54da-3681-4420-8c50-e4fdb7144bb1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1024, out_features=8, bias=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f61a21e8-ea5a-4ff5-a99c-8bd5f2d248cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(preprocessor_config_json, 'r') as f:\n",
    "  vit_prepro_config_args = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b0736cc-081e-46aa-adf4-81359e6d8de4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"feature_extractor_type\": \"ViTFeatureExtractor\",\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractor = ViTImageProcessor(**vit_prepro_config_args)\n",
    "extractor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782902f3-ce91-49f2-8260-da9da6e181cb",
   "metadata": {},
   "source": [
    "---\n",
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "781666dc-2a9d-4349-afcb-6facc830044d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_fn(filename):\n",
    "    # <BIOPSY_PROCEDURE>_<TUMOR_CLASS>_<TUMOR_TYPE>-<YEAR>-<SLIDE_ID>-<MAG>-<SEQ>\n",
    "    parsed = filename[:-4].replace('-', '_').split('_')\n",
    "    parsed.append(filename)\n",
    "    return parsed\n",
    "\n",
    "def parse_type_id(filename):\n",
    "    parsed = parse_fn(filename)\n",
    "    label_maps = {'F':'1', 'DC':'2', 'PC':'3', 'PT':'4', 'MC':'5', 'LC':'6', 'A':'7', 'TA':'8'}\n",
    "    return int(label_maps[parsed[2]])-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee10cc64-6892-4415-b740-e39c64bd46aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HistoDataset(Dataset):\n",
    "    def __init__(self, img_dir, train=False, predict=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.train = train\n",
    "        self.predict = predict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(path=self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.listdir(path=self.img_dir)[idx]\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "        image = read_image(img_path)\n",
    "        if self.train:\n",
    "            composed = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.ColorJitter(brightness=.1, contrast=0, saturation=.1, hue=.1), \n",
    "                                transforms.RandomHorizontalFlip(0.3), \n",
    "                                transforms.RandomVerticalFlip(0.3),\n",
    "                                transforms.RandomRotation(30),\n",
    "                                transforms.TrivialAugmentWide(),\n",
    "                                transforms.RandomApply(transforms=[transforms.RandomCrop(size=(128, 128))], p=0.5)\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize(mean=extractor.image_mean, std=extractor.image_std), \n",
    "                                transforms.RandomErasing(0.5),\n",
    "                                ])\n",
    "            image = composed(image)\n",
    "        if self.predict:\n",
    "            label='Unknown'\n",
    "        else:\n",
    "            label = parse_type_id(filename)\n",
    "        #image_features = extractor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        return {'pixel_values':image, 'label':label, 'filename':filename}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "70009506-6c31-4b74-b6cf-86c6de91249f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "histoDataset_train = HistoDataset(input_train, train=True)\n",
    "#train_dataset, _, _ = random_split(histoDataset_train, [0.70, 0.20, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "train_dataset, _ = random_split(histoDataset_train, [0.78, 0.22], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "465bead8-5881-4eb9-8987-e5da103df1f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "histoDataset_eval_chal = HistoDataset(input_train, train=False)\n",
    "#_, eval_dataset, challenge_dataset = random_split(histoDataset_eval_chal, [0.70, 0.20, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "_, eval_dataset = random_split(histoDataset_eval_chal, [0.78, 0.22], generator=torch.Generator().manual_seed(42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed1a7697-0046-40c4-ac08-cc07557a39a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_dataset[0]['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "cc2b555c-2c14-4350-9110-dc62a07c29ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train_dataset[0]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "28fe117c-2450-4df9-ace6-3482ccbddc4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.imshow(train_dataset[0]['pixel_values'].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "26aed957-58d5-4ddd-91a1-433cdb9b155a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plt.imshow(eval_dataset[0]['pixel_values'].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bc751406-9425-4b9f-9dec-4b3000d42707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 92)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(eval_dataset)#, len(challenge_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d965e986-1fdf-4692-a119-0feaf1d3ad82",
   "metadata": {},
   "source": [
    "---\n",
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f951e05-7a6b-48e0-8641-0a9085eb5198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#checkpoint_path = torch.load(os.path.join(home, \"data\", 'output/swin/checkpoint-1300-reproduce-layer1_2_3_4_retrain/pytorch_model.bin'))\n",
    "#model.load_state_dict(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b001489-4376-4d88-bbab-8531df2ef1ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To unfreeze some selected layers\n",
    "#len(pretrained_model.swin.encoder.layers) ==> 4 layers\n",
    "for i in range(0, 4):\n",
    "    for param in model.swin.encoder.layers[i].parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d324cd5f-3c3c-4fb8-a8b4-763506837bcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# To check if the earlier layers are frozen\n",
    "for param in model.swin.encoder.layers[1].parameters():\n",
    "    print(param.requires_grad)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b5bce524-4193-4258-b344-7e615290eee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoTrainer(Trainer):\n",
    "    def compute_label_counts(self):\n",
    "        \"\"\"Compute the number of samples per class in the training dataset.\"\"\"\n",
    "        label_counts = {}\n",
    "        for elt in self.train_dataset:\n",
    "            label = elt['label']\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += 1\n",
    "            else:\n",
    "                label_counts[label] = 1\n",
    "        label_counts = dict(sorted(label_counts.items(), key=lambda x: x[0]))\n",
    "        return label_counts\n",
    "    \n",
    "    def get_train_dataloader(self):\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "        if isinstance(train_dataset, Dataset):\n",
    "            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        else:\n",
    "            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "            \n",
    "        '''\n",
    "        label_counts = self.compute_label_counts()\n",
    "        max_samples = max(label_counts.values())\n",
    "        classes_weights = {k: max_samples/v for k, v in label_counts.items()}\n",
    "        #print(classes_weights)\n",
    "        sample_weights = torch.DoubleTensor([classes_weights[elt['label']] for elt in train_dataset]).cuda()\n",
    "        '''\n",
    "        sample_weights = torch.DoubleTensor([1 for elt in train_dataset]).cuda()\n",
    "        \n",
    "        # Calculate class weights based on the frequency of each class in the training dataset\n",
    "        #sample_weights = torch.tensor(list(classes_weights.values()), dtype=torch.float).cuda()\n",
    "        # Create a weighted sampler that oversamples the minority class during training\n",
    "        train_sampler = WeightedRandomSampler(sample_weights, len(self.train_dataset))\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self._train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "        return train_dataloader\n",
    "    \n",
    "    # If weights for classes are needed\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        loss_fct = CrossEntropyLoss(weight=torch.tensor([0.2, 0.1, 0.1, 0.1, 0.2, 0.1, 0.1, 0.1]).cuda())\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ce0f5077-9f6d-444f-83c8-6c4d3c1d0e36",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/output/swin',          # output directory\n",
    "    num_train_epochs=128,              # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    #warmup_ratio=0.01,\n",
    "    warmup_steps=50,                # number of warmup steps for learning rate scheduler\n",
    "    logging_dir='../data/log',            # directory for storing logs\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    remove_unused_columns = False,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=5e-5,\n",
    "    gradient_accumulation_steps=4,\n",
    "    #ignore_data_skip=True,\n",
    "    #resume_from_checkpoint=True,\n",
    "    #no_cuda=True\n",
    ")\n",
    "\n",
    "trainer = Trainer( #HistoTrainer\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,            # evaluation dataset\n",
    "    data_collator=DefaultDataCollator(return_tensors=\"pt\"),\n",
    "    tokenizer=extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7ded0b56-5473-41c4-9e18-e4078e7b9b58",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#trainer.compute_label_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d77fe-b8d8-437d-906e-29b7b543c43d",
   "metadata": {},
   "source": [
    "in our prediction file (computed with hugging face which has good results) :\\\n",
    "0    17\\\n",
    "1    54\\\n",
    "2    29\\\n",
    "3    25\\\n",
    "4    25\\\n",
    "6    56\\\n",
    "7     1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d46e9f0c-8cba-4421-971d-8c1ab1adecb5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3086"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to avoid errors such as  :\n",
    "# OutOfMemoryError: CUDA out of memory. Tried to allocate 14.00 MiB (GPU 0; 8.00 GiB total capacity; 6.97 GiB already allocated; 0 bytes free; 7.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "db239bc5-31aa-45b9-9abf-7f9381dc6f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 330\n",
      "  Num Epochs = 128\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1280\n",
      "  Number of trainable parameters = 86742848\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (16) must match the size of tensor b (49) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[123], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#checkpoint_path = os.path.join(home, \"data\", 'output/swin/checkpoint-1300')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#trainer.train(checkpoint_path)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1542\u001b[0m )\n\u001b[1;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\trainer.py:1791\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1789\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1791\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1794\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1795\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1796\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1797\u001b[0m ):\n\u001b[0;32m   1798\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1799\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\trainer.py:2539\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2536\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2538\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2539\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2542\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\trainer.py:2571\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2569\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2570\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2571\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   2572\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2573\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:1191\u001b[0m, in \u001b[0;36mSwinForImageClassification.forward\u001b[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1183\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1185\u001b[0m \u001b[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1191\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mswin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1199\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1201\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(pooled_output)\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:998\u001b[0m, in \u001b[0;36mSwinModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    994\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdepths))\n\u001b[0;32m    996\u001b[0m embedding_output, input_dimensions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos)\n\u001b[1;32m--> 998\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    999\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1001\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1002\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1003\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1004\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1005\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1007\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1008\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm(sequence_output)\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:829\u001b[0m, in \u001b[0;36mSwinEncoder.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, output_hidden_states, output_hidden_states_before_downsampling, always_partition, return_dict)\u001b[0m\n\u001b[0;32m    825\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    826\u001b[0m         create_custom_forward(layer_module), hidden_states, input_dimensions, layer_head_mask\n\u001b[0;32m    827\u001b[0m     )\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 829\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    834\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:747\u001b[0m, in \u001b[0;36mSwinStage.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks):\n\u001b[0;32m    745\u001b[0m     layer_head_mask \u001b[38;5;241m=\u001b[39m head_mask[i] \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 747\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dimensions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malways_partition\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    751\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    753\u001b[0m hidden_states_before_downsampling \u001b[38;5;241m=\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:677\u001b[0m, in \u001b[0;36mSwinLayer.forward\u001b[1;34m(self, hidden_states, input_dimensions, head_mask, output_attentions, always_partition)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    675\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m attn_mask\u001b[38;5;241m.\u001b[39mto(hidden_states_windows\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 677\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    681\u001b[0m attention_output \u001b[38;5;241m=\u001b[39m attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    683\u001b[0m attention_windows \u001b[38;5;241m=\u001b[39m attention_output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, channels)\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:552\u001b[0m, in \u001b[0;36mSwinAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    546\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    547\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    550\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    551\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 552\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    553\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    554\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\models\\swin\\modeling_swin.py:475\u001b[0m, in \u001b[0;36mSwinSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    470\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m relative_position_bias\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    472\u001b[0m )\n\u001b[0;32m    474\u001b[0m relative_position_bias \u001b[38;5;241m=\u001b[39m relative_position_bias\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m--> 475\u001b[0m attention_scores \u001b[38;5;241m=\u001b[39m \u001b[43mattention_scores\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mrelative_position_bias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;66;03m# Apply the attention mask is (precomputed for all layers in SwinModel forward() function)\u001b[39;00m\n\u001b[0;32m    479\u001b[0m     mask_shape \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (16) must match the size of tensor b (49) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "#checkpoint_path = os.path.join(home, \"data\", 'output/swin/checkpoint-1300')\n",
    "#trainer.train(checkpoint_path)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d75b96f-0dd6-4d55-b47b-ccada029e62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913cdc77-afaf-4805-aa02-6cb4151e5fd8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Evaluating how good the training went.\n",
    "\n",
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "61a97d2f-b054-4f11-93ae-2e2feed04c94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case of overfitting to check another checkpoint\n",
    "checkpoint_path = torch.load(os.path.join(home, \"data\", 'output/swin/checkpoint-5100/pytorch_model.bin'))\n",
    "model.load_state_dict(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "795aebb9-5ebf-45c1-93f4-19d6cc356689",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "train_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in train_dataset:\n",
    "        train_labels.append(data['label'])\n",
    "        x = data['pixel_values'][None, :].to(device) # important if inputs and weights are not on the same processing unit\n",
    "        train_preds.append(model(x).logits.argmax(-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9cdf32b-1f09-43a2-9146-1159b64b06e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 35,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0, 66,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, 43,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 45,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0, 13,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0, 55,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0, 11]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.tensor(train_labels)\n",
    "preds = torch.tensor(train_preds)\n",
    "metric = MulticlassConfusionMatrix(num_classes=8)\n",
    "metric(preds, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280052b5-4226-4fb7-baf4-072df17712ef",
   "metadata": {},
   "source": [
    "It went pretty well !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f247a82f-1b29-44e0-b0c7-bd719b9b5bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_labels = []\n",
    "eval_preds = []\n",
    "with torch.no_grad():\n",
    "    for data in eval_dataset:\n",
    "        eval_labels.append(data['label'])\n",
    "        x = data['pixel_values'][None, :].to(device) # important if inputs and weights are not on the same processing unit\n",
    "        eval_preds.append(model(x).logits.argmax(-1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2fc9615c-c4a5-4c18-89f7-a2c85d36ce44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 11,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0, 26,  0,  0,  0,  0,  0],\n",
       "        [ 1,  0,  0, 13,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 13,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  3,  0,  0],\n",
       "        [ 0,  0,  0,  1,  0,  0,  9,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  5]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = MulticlassConfusionMatrix(num_classes=8)\n",
    "metric(torch.tensor(eval_preds), torch.tensor(eval_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5a5433-dd92-4efe-a2d8-04be991afe3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Validation metrics bundle\n",
    "\n",
    "Obtain the results on the autotrain page :\n",
    "\n",
    "    Loss: 0.179\n",
    "    Accuracy: 0.966\n",
    "    Macro F1: 0.959\n",
    "    Micro F1: 0.966\n",
    "    Weighted F1: 0.966\n",
    "    Macro Precision: 0.969\n",
    "    Micro Precision: 0.966\n",
    "    Weighted Precision: 0.969\n",
    "    Macro Recall: 0.954\n",
    "    Micro Recall: 0.966\n",
    "    Weighted Recall: 0.966\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7f2a24af-4dd6-458e-a1b0-3e1451c0f058",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0,\n",
       " 'f1_macro': 1.0,\n",
       " 'f1_micro': 1.0,\n",
       " 'f1_weighted': 1.0,\n",
       " 'precision_macro': 1.0,\n",
       " 'precision_micro': 1.0,\n",
       " 'precision_weighted': 1.0,\n",
       " 'recall_macro': 1.0,\n",
       " 'recall_micro': 1.0,\n",
       " 'recall_weighted': 1.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def compute_metrics (labels, preds):\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=preds, references = labels))\n",
    "    results.update({'f1_macro':f1_metric.compute(predictions=preds, references = labels, average=\"macro\")['f1']})\n",
    "    results.update({'f1_micro':f1_metric.compute(predictions=preds, references = labels, average=\"micro\")['f1']})\n",
    "    results.update({'f1_weighted':f1_metric.compute(predictions=preds, references = labels, average=\"weighted\")['f1']})\n",
    "    results.update({'precision_macro':precision_metric.compute(predictions=preds, references = labels, average=\"macro\")['precision']})\n",
    "    results.update({'precision_micro':precision_metric.compute(predictions=preds, references = labels, average=\"micro\")['precision']})\n",
    "    results.update({'precision_weighted':precision_metric.compute(predictions=preds, references = labels, average=\"weighted\")['precision']})\n",
    "    results.update({'recall_macro':recall_metric.compute(predictions=preds, references = labels, average=\"macro\")['recall']})\n",
    "    results.update({'recall_micro':recall_metric.compute(predictions=preds, references = labels, average=\"micro\")['recall']})\n",
    "    results.update({'recall_weighted':recall_metric.compute(predictions=preds, references = labels, average=\"weighted\")['recall']})\n",
    "    return results\n",
    "\n",
    "compute_metrics(torch.tensor(train_labels), torch.tensor(train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "72d49c86-951e-47ab-ad96-0efb6f82d909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9782608695652174,\n",
       " 'f1_macro': 0.9785401002506267,\n",
       " 'f1_micro': 0.9782608695652174,\n",
       " 'f1_weighted': 0.9782336275471287,\n",
       " 'precision_macro': 0.9797077922077922,\n",
       " 'precision_micro': 0.9782608695652174,\n",
       " 'precision_weighted': 0.9792490118577075,\n",
       " 'recall_macro': 0.9785714285714286,\n",
       " 'recall_micro': 0.9782608695652174,\n",
       " 'recall_weighted': 0.9782608695652174}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(torch.tensor(eval_labels), torch.tensor(eval_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2c70da-3978-4ea9-b17c-4c0669f7df48",
   "metadata": {},
   "source": [
    "## Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe78fd2d-7426-4c3c-86ee-5b2f3b32beac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>procedure</th>\n",
       "      <th>id</th>\n",
       "      <th>filename</th>\n",
       "      <th>type_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SOB</td>\n",
       "      <td>1</td>\n",
       "      <td>SOB_1.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SOB</td>\n",
       "      <td>10</td>\n",
       "      <td>SOB_10.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SOB</td>\n",
       "      <td>100</td>\n",
       "      <td>SOB_100.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SOB</td>\n",
       "      <td>101</td>\n",
       "      <td>SOB_101.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SOB</td>\n",
       "      <td>102</td>\n",
       "      <td>SOB_102.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  procedure   id     filename  type_id\n",
       "0       SOB    1    SOB_1.png        0\n",
       "1       SOB   10   SOB_10.png        0\n",
       "2       SOB  100  SOB_100.png        0\n",
       "3       SOB  101  SOB_101.png        0\n",
       "4       SOB  102  SOB_102.png        0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wd = os.path.join(os.getcwd(), '..')\n",
    "data = os.path.join(wd, 'data', 'input')\n",
    "image_test_path = os.path.join(data, 'Test')\n",
    "\n",
    "# Test images\n",
    "images_test = os.listdir(path=image_test_path)\n",
    "submission_path = os.path.join(wd, 'data', 'output', 'submission', 'pred_swim_20230306_1408.csv')\n",
    "\n",
    "def parseTest_fn(filename):\n",
    "    # <BIOPSY_PROCEDURE>_<ID>\n",
    "    parsed = filename[:-4].split('_')\n",
    "    parsed.append(filename)\n",
    "    return parsed\n",
    "\n",
    "columns = ['procedure', 'id', 'filename']\n",
    "\n",
    "df_test = pd.DataFrame(list(map(parseTest_fn, images_test)), columns=columns)\n",
    "df_test['type_id'] = 0\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "66db0037-7585-4291-a2f6-dee1a0a505a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = HistoDataset(image_test_path, predict=True)\n",
    "types_test = []\n",
    "for data in test_data:\n",
    "    inputs = extractor(data['pixel_values'], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(inputs['pixel_values'].cuda()).logits\n",
    "    predicted_label = logits.argmax(-1).item()\n",
    "    types_test.append(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "640a6fec-56a4-444b-a92b-1d817729ea0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test['type_id'] = types_test\n",
    "df_pred = df_test[['id', 'type_id']]\n",
    "df_pred.to_csv(submission_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807df17-b30e-44f4-813a-63501c2ffdb3",
   "metadata": {},
   "source": [
    "## Compare Auto and Microsoft base\n",
    "\n",
    "Where are the differences in weights ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f71c91dc-d5c0-4f72-9090-e2b98500f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretrained_model = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224\")\n",
    "#pretrained_model = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "3846576b-d01f-4ac0-a704-7f8c594825e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checkpoint_path = torch.load(os.path.join(home, \"data\", 'output/swin/checkpoint-200/pytorch_model.bin'))\n",
    "#model.load_state_dict(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "fabbaaeb-43b0-45d4-aae3-bfb87f234ee3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\Shadow/.cache\\huggingface\\hub\\models--JoffreyMa--autotrain-histopathological_image_classification-3393093038\\snapshots\\ce7023a6fa2db96df4164220b0bca00a61b32548\\config.json\n",
      "Model config SwinConfig {\n",
      "  \"_name_or_path\": \"JoffreyMa/autotrain-histopathological_image_classification-3393093038\",\n",
      "  \"architectures\": [\n",
      "    \"SwinForImageClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"depths\": [\n",
      "    2,\n",
      "    2,\n",
      "    18,\n",
      "    2\n",
      "  ],\n",
      "  \"drop_path_rate\": 0.1,\n",
      "  \"embed_dim\": 128,\n",
      "  \"encoder_stride\": 32,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"1\",\n",
      "    \"1\": \"2\",\n",
      "    \"2\": \"3\",\n",
      "    \"3\": \"4\",\n",
      "    \"4\": \"5\",\n",
      "    \"5\": \"6\",\n",
      "    \"6\": \"7\",\n",
      "    \"7\": \"8\"\n",
      "  },\n",
      "  \"image_size\": 224,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"1\": \"0\",\n",
      "    \"2\": \"1\",\n",
      "    \"3\": \"2\",\n",
      "    \"4\": \"3\",\n",
      "    \"5\": \"4\",\n",
      "    \"6\": \"5\",\n",
      "    \"7\": \"6\",\n",
      "    \"8\": \"7\"\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_length\": 128,\n",
      "  \"mlp_ratio\": 4.0,\n",
      "  \"model_type\": \"swin\",\n",
      "  \"num_channels\": 3,\n",
      "  \"num_heads\": [\n",
      "    4,\n",
      "    8,\n",
      "    16,\n",
      "    32\n",
      "  ],\n",
      "  \"num_layers\": 4,\n",
      "  \"out_features\": null,\n",
      "  \"padding\": \"max_length\",\n",
      "  \"patch_size\": 4,\n",
      "  \"path_norm\": true,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qkv_bias\": true,\n",
      "  \"stage_names\": [\n",
      "    \"stem\",\n",
      "    \"stage1\",\n",
      "    \"stage2\",\n",
      "    \"stage3\",\n",
      "    \"stage4\"\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_absolute_embeddings\": false,\n",
      "  \"window_size\": 7\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\Shadow/.cache\\huggingface\\hub\\models--JoffreyMa--autotrain-histopathological_image_classification-3393093038\\snapshots\\ce7023a6fa2db96df4164220b0bca00a61b32548\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing SwinForImageClassification.\n",
      "\n",
      "All the weights of SwinForImageClassification were initialized from the model checkpoint at JoffreyMa/autotrain-histopathological_image_classification-3393093038.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use SwinForImageClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "#access_token = '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'\n",
    "#autotrain_model = AutoModelForImageClassification.from_pretrained(\"JoffreyMa/autotrain-histopathological_image_classification-3393093038\", use_auth_token=access_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "439ef77d-4a4c-4a31-83d5-02f7482e17b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For layer 0 : 0.00018857356917578727\n",
      "For layer 1 : 0.0001739619328873232\n",
      "For layer 2 : 0.00023301976034417748\n",
      "For layer 3 : 0.0006321489927358925\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    # Get the model parameters\n",
    "    params1 = model.swin.encoder.layers[i].state_dict()\n",
    "    params2 = autotrain_model.swin.encoder.layers[i].state_dict()\n",
    "\n",
    "    # Compare the weight tensors layer by layer\n",
    "    means = []\n",
    "    for layer_name, _ in params1.items():\n",
    "        diff = torch.abs(params1[layer_name].cpu() - params2[layer_name]).float()\n",
    "        mean_diff = torch.mean(diff)\n",
    "        means.append(mean_diff)\n",
    "        #print(f\"Layer name: {layer_name}\")\n",
    "        #print(f\"Mean absolute difference: {mean_diff:.6f}\")\n",
    "    print(f'For layer {i} : {np.mean(means)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "32cd8e9c-bba7-4f1d-8d96-d4580326b335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col2\n",
      "1    47\n",
      "2    28\n",
      "3    35\n",
      "4    20\n",
      "5    38\n",
      "6     1\n",
      "7    36\n",
      "8     2\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file into pandas dataframe\n",
    "df = pd.read_csv( os.path.join(home, \"data/output/submission/pred_swim_20230305_0925.csv\"), header=None)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = [\"col1\", \"col2\"]\n",
    "\n",
    "# Count the number of rows for each distinct value in the second column\n",
    "counts = df.groupby(\"col2\").size()\n",
    "\n",
    "# Print the result\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a714f13-d542-4c51-a09d-25a2f85ae243",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col2\n",
      "1    17\n",
      "2    54\n",
      "3    29\n",
      "4    25\n",
      "5    25\n",
      "7    56\n",
      "8     1\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file into pandas dataframe\n",
    "df = pd.read_csv( os.path.join(home, \"data/output/submission/pred_swim_20231002.csv\"), header=None)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = [\"col1\", \"col2\"]\n",
    "\n",
    "# Count the number of rows for each distinct value in the second column\n",
    "counts = df.groupby(\"col2\").size()\n",
    "\n",
    "# Print the result\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "438e23fd-58e1-49c1-896c-5ddb4fa56292",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "col2\n",
      "1    50\n",
      "2    27\n",
      "3    34\n",
      "4    27\n",
      "5    21\n",
      "6     8\n",
      "7    24\n",
      "8    16\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file into pandas dataframe\n",
    "df = pd.read_csv( os.path.join(home, \"data/output/submission/pred_swim_20230306_1408.csv\"), header=None)\n",
    "\n",
    "# Rename columns\n",
    "df.columns = [\"col1\", \"col2\"]\n",
    "\n",
    "# Count the number of rows for each distinct value in the second column\n",
    "counts = df.groupby(\"col2\").size()\n",
    "\n",
    "# Print the result\n",
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f9183-018c-4cbc-a5b6-ec45af4fbe8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
