{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab48e23a-f21d-4e8c-b36d-d4d2b671aaed",
   "metadata": {},
   "source": [
    "# Data Challenge - Joffrey Ma\n",
    "\n",
    "This notebook contains the code and report for the Data Challenge of the Mastères IA & BGD at Télécom Paris.\\\n",
    "Due by March 10th, 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f274e-63d6-4d8d-b1db-c860df701b03",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "The challenge is an image classification task, possibly requiring heavy image preprocessing to get meaningful and relevant features.\n",
    "\n",
    "Going head down into feature engineering and blind model testing does not bring prospects of obtaining a high score on such a challenge.\\\n",
    "I decided to start by trying out one the latest Hugging Face service : AutoTrain for image classification [https://huggingface.co/blog/autotrain-image-classification]. \\\n",
    "It automatically selects relevant models to try out on a dataset uploaded by the user.\\\n",
    "Initial goals are : \n",
    "* to try out the latest machine learning architecture for image classification that hold top positions on benchmarks [https://paperswithcode.com/task/image-classification].\n",
    "* set a baseline and a foundation for improvements if results are somewhat good compare to the dozen people that already submitted in the past week. \n",
    "\n",
    "In its free version, a dataset of under 500 images is allowed. Right what is necessary for the challenge. Data splitting can be left to AutoTrain. As regards models, up to 5 can be compared in the free version. Model selection can also be left to AutoTrain.\\\n",
    "Once data is splitted and models selected, AutoTrain trains them. It decides on its own when to stop training and notice the user of each model training termination.\n",
    "From the autotrained models, I download the best in term of F1 and evaluate the test dataset and submit on the datachallenge platform.\n",
    "\n",
    "This experiment fulfilled my goals :\n",
    "* Swin, ViT, BEiT and ResNet are among selected model, and they show high performances on benchmark of different image classification tasks.\n",
    "* the associated submission ranked me first, slightly above the previous first.\n",
    "Plus, it indicates that fine-tuning can really work on very small datasets.  \n",
    "\n",
    "Now I have my starting point with a Swin vision transformer as Hugging Face delivers the model architecture and weights, the actual image processor and its configuration file.\\\n",
    "However, simply using the autotrained Swin is not satisfactory. At least I should be able to reproduce the results locally, hence proving chance has nothing to do with them. Once the performances are reproduced, I can ponder on how to improve my process if possible.\\\n",
    "Meanwhile if my rank goes below the top 5, I would consider exploring a completely different path. If it remains in the top 5, I stick to the ongoing path. ==> This is what happened !\n",
    "\n",
    "In this notebook will be presented a simple code to generate data in the format expected by Hugging Face which turns it into a dataset available on its hub [https://huggingface.co/datasets/JoffreyMa/autotrain-data-histopathological_image_classification].\\\n",
    "Then show how to download the autotrained model [https://huggingface.co/JoffreyMa/autotrain-histopathological_image_classification-3393093038] and use it to make a submission.\\\n",
    "I resume with how to locally obtain similar results, even though Hugging Face AutoTrain does not provide information on its training process.\\\n",
    "Afterwards I present another technic explored and end with leads and prospects of improvements.\\\n",
    "As we go through I explain what is necessary (model architecture, augmentation technics, inner working of Hugging Face Transformers) \n",
    "\n",
    "In other words, this notebook summary is the following :\n",
    "1. Hugging Face Autotrain\n",
    "    1. Format data\n",
    "    1. API\n",
    "    1. Apply\n",
    "1. Reproduce locally\n",
    "    1. Configure\n",
    "    1. Augment data\n",
    "    1. Balance data\n",
    "    1. Train\n",
    "    1. Evaluate\n",
    "    1. Predict\n",
    "1. Improve\n",
    "    1. Mitigate overfitting\n",
    "    1. Task related\n",
    "    1. Time\n",
    "\n",
    "And I will wrap it with a paragraph about some exploration I made and a conclusion of the project.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "141f169c-b94b-4720-99da-4e86f8b7a1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for the notebook\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import AutoFeatureExtractor, AutoModelForImageClassification, SwinConfig, SwinForImageClassification, ViTImageProcessor, Trainer, TrainingArguments, DefaultDataCollator\n",
    "\n",
    "import torch\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset, random_split, DataLoader, WeightedRandomSampler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import gc\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98eb376-c56b-4008-b68a-1a971ddfb374",
   "metadata": {},
   "source": [
    "# 1. Hugging Face Autotrain\n",
    "\n",
    "In order to use AutoTrain, one must comply by uploading a folder containing all the images for training and another csv file.\\ \n",
    "\n",
    "Exemple, content of histopathological_image_classification.csv :\\\n",
    "image_relpath,label\\\n",
    "SOB_B_A-14-22549AB-100-001.png,7\\\n",
    "SOB_B_A-14-22549AB-100-002.png,7\\\n",
    "SOB_B_A-14-22549AB-100-003.png,7\\\n",
    "...\n",
    "\n",
    "As we can see the csv has 2 columns, one for the image filename and another for the associated label.\n",
    "\n",
    "## A. Format data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cee8a8-a0dd-4e8f-ab5d-9840556f6df3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paths for this part of the notebook\n",
    "wd = os.path.join(os.getcwd(), '..')\n",
    "data = os.path.join(wd, 'data', 'input')\n",
    "image_train_path = os.path.join(data, 'Train')\n",
    "image_test_path = os.path.join(data, 'Test')\n",
    "\n",
    "hf_csv_output_path = os.path.join(wd, 'data', 'output', 'huggingface_autotrain', 'histopathological_image_classification.csv')\n",
    "hf_submission_path = os.path.join(wd, 'data', 'output', 'submission', 'pred_swin_hf_autotrain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd389bb5-dc18-48d6-ae6e-b40b9df2e51f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  procedure class type magnification    slide  mag  seq  \\\n",
      "0       SOB     B    A            14  22549AB  100  001   \n",
      "1       SOB     B    A            14  22549AB  100  002   \n",
      "2       SOB     B    A            14  22549AB  100  003   \n",
      "\n",
      "                         filename type_id  \n",
      "0  SOB_B_A-14-22549AB-100-001.png       7  \n",
      "1  SOB_B_A-14-22549AB-100-002.png       7  \n",
      "2  SOB_B_A-14-22549AB-100-003.png       7  \n"
     ]
    }
   ],
   "source": [
    "# Code from the tutorial to process the data\n",
    "\n",
    "# Get images list from folder using os.listdir\n",
    "images = os.listdir(path=image_train_path)\n",
    "images = [image for image in images if Path(image).suffix == '.png']\n",
    "\n",
    "# Extract info from the filenames\n",
    "def parse_fn(filename):\n",
    "    # <BIOPSY_PROCEDURE>_<TUMOR_CLASS>_<TUMOR_TYPE>-<YEAR>-<SLIDE_ID>-<MAG>-<SEQ>\n",
    "    parsed = filename[:-4].replace('-', '_').split('_')\n",
    "    parsed.append(filename)\n",
    "    return parsed\n",
    "\n",
    "columns = ['procedure', 'class', 'type', 'magnification', 'slide', 'mag', 'seq', 'filename']\n",
    "\n",
    "# Store info about files in a pandas DataFrame \n",
    "df = pd.DataFrame(list(map(parse_fn, images)), columns=columns)\n",
    "label_types = df['type'].unique()\n",
    "# Maps label names to indices\n",
    "label_maps = {'F':'1', 'DC':'2', 'PC':'3', 'PT':'4', 'MC':'5', 'LC':'6', 'A':'7', 'TA':'8'}\n",
    "df['type_id'] = df['type'].apply(lambda x: label_maps[x])\n",
    "\n",
    "# Glimpse at the obtained info\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24cc0a3f-a8af-48aa-8fd9-a5f5c0dd0399",
   "metadata": {},
   "source": [
    "I notice that procedure, magnification, mag always have the same values.\\\n",
    "We have different sequences for each slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40374c9c-158e-4f7e-a163-1cdafa9d8ff5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    image_relpath label\n",
      "0  SOB_B_A-14-22549AB-100-001.png     7\n",
      "1  SOB_B_A-14-22549AB-100-002.png     7\n",
      "2  SOB_B_A-14-22549AB-100-003.png     7\n"
     ]
    }
   ],
   "source": [
    "# Select filename and label indice  \n",
    "hf = df[['filename', 'type_id']].copy()\n",
    "hf.columns = ['image_relpath', 'label']\n",
    "# Save csv, ready for use on Hugging Face \n",
    "hf.to_csv(hf_csv_output_path, index=False)\n",
    "# Glimpse at the csv \n",
    "print(hf.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60245f1-4b06-4cd0-8bff-c26c6fa15d22",
   "metadata": {},
   "source": [
    "## B. API\n",
    "\n",
    "Let's use the best autotrained model.\n",
    "We must download it along with its extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11c4b962-cdca-4172-993a-86f546330c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadow\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "access_token = 'your_access_token_from_hugging_face' # contact me if necessary\n",
    "\n",
    "extractor = AutoFeatureExtractor.from_pretrained(\"JoffreyMa/autotrain-histopathological_image_classification-3393093038\", use_auth_token=access_token)\n",
    "model = AutoModelForImageClassification.from_pretrained(\"JoffreyMa/autotrain-histopathological_image_classification-3393093038\", use_auth_token=access_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada31ae-cc8d-4f71-a30e-15de9de9ea9b",
   "metadata": {},
   "source": [
    "The extracted model is a Swin transformer for image classification.\n",
    "More information on that model can be found at [https://github.com/microsoft/Swin-Transformer] and [https://arxiv.org/abs/2103.14030]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e297f565-e1b9-4893-a92c-29512ef653ed",
   "metadata": {},
   "source": [
    "## C. Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacf55ce-b499-449a-8319-c25de901dafc",
   "metadata": {},
   "source": [
    "Declare a small dataset class to extract images and their filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8751cda8-856b-4f77-9f65-3633fdf9121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFHistoDataset(Dataset):\n",
    "    def __init__(self, img_dir):\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(path=self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.listdir(path=self.img_dir)[idx]\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "        image = read_image(img_path)\n",
    "        return image, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5c0b18f1-6f0b-4223-af53-b30666ad4917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = HFHistoDataset(image_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa84977-b9e0-4a0d-b9ec-906deafa4528",
   "metadata": {
    "tags": []
   },
   "source": [
    "Recycle code from datachallenge.enst.fr tutorial to prepare a submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7b53ef6-cb1c-466a-8820-e3852a484635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images for the test set:  207\n"
     ]
    }
   ],
   "source": [
    "# Test images\n",
    "images_test = os.listdir(path=image_test_path)\n",
    "\n",
    "# Number of images\n",
    "print(\"Number of images for the test set: \", len(images_test))\n",
    "\n",
    "columns = ['procedure', 'id', 'filename']\n",
    "\n",
    "df_test = pd.DataFrame(list(map(parse_fn, images_test)), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dcc640d-3c95-495e-a6b5-d0f86233bac4",
   "metadata": {},
   "source": [
    "Predict on each image of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4347cab2-a60c-4359-b8a6-9eda97cca72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "types_test = []\n",
    "for data in test_data:\n",
    "    inputs = extractor(data[0], return_tensors=\"pt\") # important to extract what the autotrained model needs\n",
    "    filename = data[1]\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_label = logits.argmax(-1).item()\n",
    "    types_test.append(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7054ade-6e71-4d1b-87fa-efba8fafeafd",
   "metadata": {},
   "source": [
    "Write the submission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef2bca14-7e16-4dcf-bc0b-941e06e98612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    56\n",
       "2    54\n",
       "3    29\n",
       "4    25\n",
       "5    25\n",
       "1    17\n",
       "8     1\n",
       "Name: type_id, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test['type_id'] = types_test\n",
    "df_pred = df_test[['id', 'type_id']]\n",
    "display(df_pred['type_id'].value_counts())\n",
    "df_pred.to_csv(hf_submission_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e735947e-9183-4e2f-b6d5-0e9974f3941d",
   "metadata": {},
   "source": [
    "With this submission, I get 0.778882621975 F1-score and 0.835748792271 Accuracy, ranked first when submitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb9eda9-d6fe-42d5-a027-3c650371f569",
   "metadata": {},
   "source": [
    "Now I need to reproduce the results locally, at least to prove these results are not obtained by chance. Only then can I improve on this model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a0ae84-97c4-4702-bcf4-f9b902df10bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Reproduce locally\n",
    "\n",
    "## A. Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01c88640-4724-4a81-b2b9-af0c06821e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device on which computation happens\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# paths for this part of the notebook\n",
    "config_json = os.path.join(wd, \"data\", \"model\", \"config\", \"config.json\")\n",
    "preprocessor_config_json = os.path.join(wd, \"data\", \"model\", \"config\", \"preprocessor_config.json\")\n",
    "input_train = os.path.join(wd, \"data\", \"input\", \"Train\")\n",
    "model_path = os.path.join(wd, \"data\", \"model\")\n",
    "submission_path = os.path.join(wd, 'data', 'output', 'submission', 'pred_swin_local.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74680f33-ec4e-4f75-b01a-1e1a407aeaba",
   "metadata": {},
   "source": [
    "Hugging Face gives us key characteristics of the model but not the pre-trained base model with which it fine-tunes. Fortunately, by looking at the architecture of the autotrained swin, I am able to recognize which type of Swin model was used.\\\n",
    "\n",
    "At this stage, two possible pretrained base models exist :\n",
    "* swin base trained on ImageNet 22K [https://huggingface.co/microsoft/swin-base-patch4-window7-224-in22k]\n",
    "* swin base trained on ImageNet 1K [https://huggingface.co/microsoft/swin-base-patch4-window7-224]\n",
    "\n",
    "By looking at the differences in model weights with the autotrained model, we can assume the pre-trained model used is swin base trained on ImageNet 22K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da880111-123b-43cf-a355-217e876c99be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the model\n",
    "\n",
    "# Load pre-trained model\n",
    "pretrained_model = SwinForImageClassification.from_pretrained(\"microsoft/swin-base-patch4-window7-224-in22k\")\n",
    "\n",
    "# Open configuration file given by Hugging Face\n",
    "# contains the architecture of the model\n",
    "with open(config_json, 'r') as f:\n",
    "  swin_config_args = json.load(f)\n",
    "configuration = SwinConfig(**swin_config_args)\n",
    "\n",
    "# Declare a model with regards to that config\n",
    "model = SwinForImageClassification(configuration)\n",
    "# SwinForImageClassification contains a linear (fully connected) layer \n",
    "# on top of swin and a softmax to perform classification \n",
    "\n",
    "# Swap the blank instanciated swin with the one by Microsoft  \n",
    "model.swin = pretrained_model.swin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8aa8427f-4428-44b5-9a99-f45316ffe3c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Configure the image processor\n",
    "\n",
    "# Load the image processor by Hugging Face\n",
    "# here the image processor is a brick defined in Hugging Face Transformers\n",
    "# As they put it :\n",
    "# An image processor is in charge of preparing input features for vision models\n",
    "# and post processing their outputs. This includes transformations such as resizing, \n",
    "# normalization, and conversion to PyTorch, TensorFlow, Flax and Numpy tensors. \n",
    "# It may also include model specific post-processing \n",
    "# such as converting logits to segmentation masks.\n",
    "with open(preprocessor_config_json, 'r') as f:\n",
    "  vit_prepro_config_args = json.load(f)\n",
    "\n",
    "# Declare the ViT image processor from the config\n",
    "extractor = ViTImageProcessor(**vit_prepro_config_args)\n",
    "# it does normalization, rescaling, and resizing of images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed5fa1-9b13-4a0a-9a7b-e61d0582ba67",
   "metadata": {},
   "source": [
    "## B. Augment data\n",
    "\n",
    "In order to avoid overfitting to the training data, one interesting step in our case is to augment available data. By doing so we artificially increase the volume of data and change the data seen by the model as training goes so that it does not find cheaty ways of guessing the label.\n",
    "\n",
    "A lot of augmentation technics exists, among the ones for images, I chose some that I think would not change the appropriate label and do not generate completely \"false\" image unrelated to our task of classifying breast tumors histology slides. \n",
    "\n",
    "Let's begin by defining our custom Dataset class which will contain code to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f53cc89-1db7-4bc5-814d-b4dd52739cf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility for the custom Dataset\n",
    "def parse_type_id(filename):\n",
    "    parsed = parse_fn(filename)\n",
    "    label_maps = {'F':'1', 'DC':'2', 'PC':'3', 'PT':'4', 'MC':'5', 'LC':'6', 'A':'7', 'TA':'8'}\n",
    "    return int(label_maps[parsed[2]])-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114a12e-9721-4288-8088-71d3f697bbd8",
   "metadata": {},
   "source": [
    "As the model ingest training data, this Dataset subclass augment them.\\\n",
    "Some augmentations require a bit of explanation.\\\n",
    "* ColorJitter randomly modifies here the brightness, saturation and hue. The idea is that images with acquired with microscope h&e staining protocol can have a lot of variance for instance the colors of nuclei (~ blue), cytoplasm (~ pink), erythrocytes (~ red).\n",
    "* slides do not have any orientation, our model should analyse images flipped or rotated anyhow\n",
    "\n",
    "Additionnaly,\n",
    "* rotations produce 0-valued pixels in the image, which could be seen as missing or masked data with which the model should deal. I even add random erasing so that missing data can happen anywhere. \n",
    "Others could be included, we will discuss that matter in the 'Improve' section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3adb6d4-1888-42fa-b770-d1c588c52d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subclass of torch Dataset \n",
    "class HistoDataset(Dataset):\n",
    "    def __init__(self, img_dir, train=False, predict=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.train = train\n",
    "        self.predict = predict\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(os.listdir(path=self.img_dir))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = os.listdir(path=self.img_dir)[idx]\n",
    "        img_path = os.path.join(self.img_dir, filename)\n",
    "        image = read_image(img_path)\n",
    "        if self.train:\n",
    "            composed = transforms.Compose([transforms.ColorJitter(brightness=.1, contrast=0, saturation=.1, hue=.1),\n",
    "                                transforms.RandomHorizontalFlip(0.3), \n",
    "                                transforms.RandomVerticalFlip(0.3),\n",
    "                                #transforms.RandomRotation(30), # transforms commented as I obtain a high score without them !\n",
    "                                #transforms.TrivialAugmentWide(),\n",
    "                                #transforms.RandomApply(transforms=[transforms.RandomResizedCrop(size=(460, 700))], p=0.5),\n",
    "                                #transforms.ToTensor(),\n",
    "                                #transforms.RandomErasing(0.5),\n",
    "                                ])\n",
    "            image = composed(image)\n",
    "        if self.predict:\n",
    "            label='Unknown'\n",
    "        else:\n",
    "            label = parse_type_id(filename)\n",
    "        image_features = extractor(image, return_tensors=\"pt\").pixel_values.squeeze(0)\n",
    "        return {'pixel_values':image_features, 'label':label, 'filename':filename}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae08f58-112f-4210-950d-941424beb106",
   "metadata": {},
   "source": [
    "Data are splitted between a training and a validation set with a ratios of 0.78, 0.22 respectively (330 and 92 images). A standard choice, close to what Hugging Face did with our data previously before AutoTrain.\\\n",
    "Both sets should be big enough to learn and evaluate our model. \\\n",
    "Other data splitting methods look less relevant at first glance. \\\n",
    "For instance, cross-validation would be too time consuming since training time is long in this notebook. More important, even if computation goes fast, I need enough data so that the remaining validation fold is representative of the training data. I might not be able to get enough folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e80c0ff-2550-42a7-bc75-7617b224e24c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 92)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Declare a train set\n",
    "# with seed=42 to reproduce it\n",
    "histoDataset_train = HistoDataset(input_train, train=True)\n",
    "train_dataset, _ = random_split(histoDataset_train, [0.78, 0.22], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "# Declare a validation set\n",
    "# Thanks to the seed I can reproduce the split \n",
    "# and get a sub HistoDataset with train set to False\n",
    "# so that no augmentation happens on the validation set\n",
    "histoDataset_eval = HistoDataset(input_train, train=False)\n",
    "_, eval_dataset = random_split(histoDataset_eval, [0.78, 0.22], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "len(train_dataset), len(eval_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7fcca0-6026-406e-bfb4-d1d8fe933314",
   "metadata": {},
   "source": [
    "## C. Balance data\n",
    "\n",
    "As seen in the tutorial, classes are imbalanced. This is an issue as the test data of the challenge might have a different distribution.\\\n",
    "Without knowing the distribution of the test data, I can try to equalize the amount of data seen in each class by the model.\\\n",
    "My hope is then to get more robust results and not over-classify as \"PC\" or under-classify as \"TA\".\n",
    "\n",
    "One way of rebalancing is to modify how to sample data to feed to the model.\\\n",
    "With Hugging Face Transformers, it can be done with a custom train data loader.\n",
    "\n",
    "As far as AutoTrain is concerned their seems to be no way of telling if such balancing procedure was performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ca9b620-f05e-484a-90be-0c1b4ccc2e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HistoTrainer(Trainer):\n",
    "    def compute_label_counts(self):\n",
    "        \"\"\"Compute the number of samples per class in the training dataset.\"\"\"\n",
    "        label_counts = {}\n",
    "        for elt in self.train_dataset:\n",
    "            label = elt['label']\n",
    "            if label in label_counts:\n",
    "                label_counts[label] += 1\n",
    "            else:\n",
    "                label_counts[label] = 1\n",
    "        label_counts = dict(sorted(label_counts.items(), key=lambda x: x[0]))\n",
    "        return label_counts\n",
    "    \n",
    "    def get_train_dataloader(self):\n",
    "        train_dataset = self.train_dataset\n",
    "        data_collator = self.data_collator\n",
    "        if isinstance(train_dataset, Dataset):\n",
    "            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n",
    "        else:\n",
    "            data_collator = self._get_collator_with_removed_columns(data_collator, description=\"training\")\n",
    "            \n",
    "        label_counts = self.compute_label_counts()\n",
    "        max_samples = max(label_counts.values())\n",
    "        classes_weights = {k: max_samples/v for k, v in label_counts.items()}\n",
    "        sample_weights = torch.DoubleTensor([classes_weights[elt['label']] for elt in train_dataset]).cuda()\n",
    "        \n",
    "        # In case we want to get the same probability for each image to get drawn\n",
    "        # sample_weights = torch.DoubleTensor([1 for elt in train_dataset]).cuda()\n",
    "        \n",
    "        # Calculate class weights based on the frequency of each class in the training dataset\n",
    "        # Create a weighted sampler that oversamples the minority classes during training\n",
    "        train_sampler = WeightedRandomSampler(sample_weights, len(self.train_dataset))\n",
    "\n",
    "        train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=self._train_batch_size,\n",
    "            sampler=train_sampler,\n",
    "            collate_fn=data_collator,\n",
    "            drop_last=self.args.dataloader_drop_last,\n",
    "            num_workers=self.args.dataloader_num_workers,\n",
    "            pin_memory=self.args.dataloader_pin_memory,\n",
    "        )\n",
    "        return train_dataloader\n",
    "    \n",
    "    # If weights for classes are needed\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "        # compute custom loss\n",
    "        # if after the training I get errors on specific classes \n",
    "        # I can adjust their importance in the loss\n",
    "        loss_fct = CrossEntropyLoss(weight=torch.tensor([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]).cuda()) # 0.1 everywhere means it is not necessary to weigh here\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea5895e-4b49-47c9-8b91-7ce42e0d5b01",
   "metadata": {},
   "source": [
    "It happens that without such balancing you can already get good results.\n",
    "\n",
    "We are going to see that with the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6d877-d259-442c-9516-27f7b6e9bd31",
   "metadata": {},
   "source": [
    "## D. Train\n",
    "\n",
    "The Transformers library contains useful objects to fine-tune pre-trained models.\\\n",
    "Especially Trainer and its TrainingArguments.\\\n",
    "They avoid writing explicitly the training loop like in regular pytorch, while taking care of the optimizer with the latest ones implemented.\n",
    "\n",
    "The main arguments to toy with are the number of training epochs, the learning rate, the number of warmup steps for the scheduler, the weight decay.\\\n",
    "I choose the number of training epochs so to try to reach overfitting the data (from experience here it varies from 32 to 256 epochs). I use a strategy of regularly saving intermediate models. Hopefully some perform well without overfitting. Beware it is greedy in memory and needs 10s of gigabytes. Checkpoints are stored in the declared output directory.\\\n",
    "Learning rate is set low at 5e-5 for fine-tuning, not to loose the profits of microsoft swin backbone.\\\n",
    "Batch sizes are the maximum values that operate on my setup. They are quite low too, which certainly has a regularizing effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f04a13d2-c624-4f82-89d3-d66f4fa347f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='../data/output/swin',          # output directory\n",
    "    num_train_epochs=128,              # total # of training epochs\n",
    "    per_device_train_batch_size=8,  # batch size per device during training\n",
    "    per_device_eval_batch_size=16,   # batch size for evaluation\n",
    "    #warmup_ratio=0.01,\n",
    "    warmup_steps=50,                # number of warmup steps for learning rate scheduler\n",
    "    logging_dir='../data/log',            # directory for storing logs\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    remove_unused_columns = False,\n",
    "    weight_decay=0.01,\n",
    "    learning_rate=5e-5,\n",
    "    #gradient_accumulation_steps=4,\n",
    "    #ignore_data_skip=True,\n",
    "    #resume_from_checkpoint=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer( #or HistoTrainer to balance the data \n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=eval_dataset,            # evaluation dataset\n",
    "    #data_collator=DefaultDataCollator(return_tensors=\"pt\"),\n",
    "    #tokenizer=extractor,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7d87b7-2687-464f-8124-a6e610f35673",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to avoid some rare memory errors during training\n",
    "# empty the unused cache memory of the device\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8d5d01e-97a6-46be-9836-980b4a7469fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shadow\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 330\n",
      "  Num Epochs = 128\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5376\n",
      "  Number of trainable parameters = 86751424\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2500' max='5376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2500/5376 1:05:19 < 1:15:12, 0.64 it/s, Epoch 59.50/128]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.775700</td>\n",
       "      <td>1.404278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.886000</td>\n",
       "      <td>0.724060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.481000</td>\n",
       "      <td>0.186720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.302700</td>\n",
       "      <td>0.383603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.151900</td>\n",
       "      <td>0.255710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.180000</td>\n",
       "      <td>0.372696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.190547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.033200</td>\n",
       "      <td>0.161963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.125390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.035100</td>\n",
       "      <td>0.046688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.018300</td>\n",
       "      <td>0.143150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.069916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.025200</td>\n",
       "      <td>0.161685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>0.105487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.123150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.079548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.109748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.015700</td>\n",
       "      <td>0.175293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.135707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.019600</td>\n",
       "      <td>0.208388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.082150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.011200</td>\n",
       "      <td>0.067705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.001400</td>\n",
       "      <td>0.079025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.115654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.322342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.029200</td>\n",
       "      <td>0.124437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.067200</td>\n",
       "      <td>0.140147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.061100</td>\n",
       "      <td>0.011958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.122846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.127470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.234178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.032300</td>\n",
       "      <td>0.714243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.028500</td>\n",
       "      <td>0.387978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.201517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.226323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.002100</td>\n",
       "      <td>0.294600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.005100</td>\n",
       "      <td>0.137406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.124356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.001800</td>\n",
       "      <td>0.169038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.159824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.244035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.132079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.213497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.000200</td>\n",
       "      <td>0.330108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.291450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.244012</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-100\n",
      "Configuration saved in ../data/output/swin\\checkpoint-100\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-200\n",
      "Configuration saved in ../data/output/swin\\checkpoint-200\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-300\n",
      "Configuration saved in ../data/output/swin\\checkpoint-300\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-400\n",
      "Configuration saved in ../data/output/swin\\checkpoint-400\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-500\n",
      "Configuration saved in ../data/output/swin\\checkpoint-500\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-600\n",
      "Configuration saved in ../data/output/swin\\checkpoint-600\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-700\n",
      "Configuration saved in ../data/output/swin\\checkpoint-700\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-800\n",
      "Configuration saved in ../data/output/swin\\checkpoint-800\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-900\n",
      "Configuration saved in ../data/output/swin\\checkpoint-900\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1000\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1000\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1100\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1100\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1200\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1200\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1300\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1300\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1400\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1400\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1500\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1500\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1500\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1600\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1600\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1600\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1700\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1700\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1700\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1800\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1800\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1800\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-1900\n",
      "Configuration saved in ../data/output/swin\\checkpoint-1900\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-1900\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-2000\n",
      "Configuration saved in ../data/output/swin\\checkpoint-2000\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-2000\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-2100\n",
      "Configuration saved in ../data/output/swin\\checkpoint-2100\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-2100\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-2200\n",
      "Configuration saved in ../data/output/swin\\checkpoint-2200\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-2200\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-2300\n",
      "Configuration saved in ../data/output/swin\\checkpoint-2300\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-2300\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n",
      "Saving model checkpoint to ../data/output/swin\\checkpoint-2400\n",
      "Configuration saved in ../data/output/swin\\checkpoint-2400\\config.json\n",
      "Model weights saved in ../data/output/swin\\checkpoint-2400\\pytorch_model.bin\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 92\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# example in case training is stopped and need to resume from a checkpoint\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# checkpoint_path = os.path.join(home, \"data\", 'output/swin/checkpoint-1300')\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# trainer.train(checkpoint_path)\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\trainer.py:1543\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1540\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1542\u001b[0m )\n\u001b[1;32m-> 1543\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\trainer.py:1858\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1856\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1858\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1860\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[0;32m   1861\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\torch\\optim\\optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\Documents\\Projets\\MastereIA\\DataChallenge\\histopatho_venv\\lib\\site-packages\\transformers\\optimization.py:361\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[0;32m    360\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m--> 361\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    362\u001b[0m denom \u001b[38;5;241m=\u001b[39m exp_avg_sq\u001b[38;5;241m.\u001b[39msqrt()\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    364\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# example in case training is stopped and need to resume from a checkpoint\n",
    "# checkpoint_path = os.path.join(home, \"data\", 'output/swin/checkpoint-1300')\n",
    "# trainer.train(checkpoint_path)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaac9bb-7153-444b-83d2-755c93941856",
   "metadata": {},
   "source": [
    "Whenever I notice some overfit I manually do an early stop.\\\n",
    "The best model is the latest with low loss on the eval set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7cd18ced-7601-490f-b180-a9913b93b11b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to C:\\Users\\Shadow\\Documents\\Projets\\MastereIA\\DataChallenge\\histopathological\\histopathological\\..\\data\\model\n",
      "Configuration saved in C:\\Users\\Shadow\\Documents\\Projets\\MastereIA\\DataChallenge\\histopathological\\histopathological\\..\\data\\model\\config.json\n",
      "Model weights saved in C:\\Users\\Shadow\\Documents\\Projets\\MastereIA\\DataChallenge\\histopathological\\histopathological\\..\\data\\model\\pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "# Save the last model obtained for good measure, it might be good\n",
    "trainer.save_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5aeb26-0d58-4b11-86ee-1d8b5eb09c2d",
   "metadata": {},
   "source": [
    "## E. Evaluate\n",
    "\n",
    "To make sure results are satisfactory the loss value is not enough. Other metrics are necessary, like the F1-Score (weighted average) chosen for the challenge.\\\n",
    "These metrics although computable at train are computed after training in this notebook.\n",
    "So it does not overload training, for which only loss interest me, plus there is a strong correlation between loss and the F1-Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e4ce5b7-16cd-41d2-998f-c20b98c3e77a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinForImageClassification(\n",
       "  (swin): SwinModel(\n",
       "    (embeddings): SwinEmbeddings(\n",
       "      (patch_embeddings): SwinPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): SwinEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (4): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (5): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (6): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (7): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (8): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (9): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (10): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (11): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (12): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (13): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (14): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (15): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (16): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (17): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinLayer(\n",
       "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=8, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in case the evaluated model is a checkpoint\n",
    "checkpoint_path = torch.load(os.path.join(wd, \"data\", 'output/swin/checkpoint-1900/pytorch_model.bin'))\n",
    "model.load_state_dict(checkpoint_path)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08b56cad-8307-4d87-8aff-1e674f4efcdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tensor_labels_preds(dataset):\n",
    "    '''Returns labels and predictions as tensors'''\n",
    "    labels = []\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in dataset:\n",
    "            labels.append(data['label'])\n",
    "            x = data['pixel_values'][None, :].to(device) # important if inputs and weights are not on the same processing unit\n",
    "            preds.append(model(x).logits.argmax(-1).item())\n",
    "    return torch.tensor(labels), torch.tensor(preds)\n",
    "\n",
    "def confusion_matrix(labels, preds):\n",
    "    metric = MulticlassConfusionMatrix(num_classes=8)\n",
    "    return metric(labels, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "378d6210-5256-465e-89ad-0bedc643730f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_labels, train_preds = tensor_labels_preds(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6966f1a8-eb54-4c4b-9f02-db4d8c59a8a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "eval_labels, eval_preds = tensor_labels_preds(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15640dc0-e351-46d5-bb31-ae43fe932897",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[62,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 35,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0, 66,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0, 43,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 44,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0, 13,  0,  0],\n",
       "        [ 0,  0,  0,  0,  1,  0, 55,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0, 11]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(train_labels, train_preds) # train_dataset is not deterministic due to augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d299a-2940-4f5a-96ca-d2ebfbb610cc",
   "metadata": {},
   "source": [
    "Apparently training went pretty good, how about the validation set ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26be3059-1803-4ba8-af4e-d361e0df180a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0, 11,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0, 26,  0,  1,  0,  0,  0],\n",
       "        [ 0,  0,  0, 14,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0, 12,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  3,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0, 10,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  5]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(eval_labels, eval_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de879fe-98b5-47df-9bf9-1471300b4315",
   "metadata": {},
   "source": [
    "Good too ! With such small data and convincing result it is hard to say on what the model could possibly perform badly.\n",
    "\n",
    "Before going onto predicting on the test data, let's check if we get metrics of the same order of magnitude as AutoTrain.\\\n",
    "These are displayed on the model page on Hugging Face, but I remind them here :\n",
    "\n",
    "    Loss: 0.179\n",
    "    Accuracy: 0.966\n",
    "    Macro F1: 0.959\n",
    "    Micro F1: 0.966\n",
    "    Weighted F1: 0.966\n",
    "    Macro Precision: 0.969\n",
    "    Micro Precision: 0.966\n",
    "    Weighted Precision: 0.969\n",
    "    Macro Recall: 0.954\n",
    "    Micro Recall: 0.966\n",
    "    Weighted Recall: 0.966\n",
    "    \n",
    "Getting all these metrics is easy with Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77852159-a46a-49fb-974e-5fdd4f45186c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def compute_metrics (labels, preds):\n",
    "    results = {}\n",
    "    results.update(accuracy_metric.compute(predictions=preds, references = labels))\n",
    "    results.update({'f1_macro':f1_metric.compute(predictions=preds, references = labels, average=\"macro\")['f1']})\n",
    "    results.update({'f1_micro':f1_metric.compute(predictions=preds, references = labels, average=\"micro\")['f1']})\n",
    "    results.update({'f1_weighted':f1_metric.compute(predictions=preds, references = labels, average=\"weighted\")['f1']})\n",
    "    results.update({'precision_macro':precision_metric.compute(predictions=preds, references = labels, average=\"macro\")['precision']})\n",
    "    results.update({'precision_micro':precision_metric.compute(predictions=preds, references = labels, average=\"micro\")['precision']})\n",
    "    results.update({'precision_weighted':precision_metric.compute(predictions=preds, references = labels, average=\"weighted\")['precision']})\n",
    "    results.update({'recall_macro':recall_metric.compute(predictions=preds, references = labels, average=\"macro\")['recall']})\n",
    "    results.update({'recall_micro':recall_metric.compute(predictions=preds, references = labels, average=\"micro\")['recall']})\n",
    "    results.update({'recall_weighted':recall_metric.compute(predictions=preds, references = labels, average=\"weighted\")['recall']})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "581c2d7c-2992-48a8-bc6d-35610f014300",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.996969696969697,\n",
       " 'f1_macro': 0.9974693794918514,\n",
       " 'f1_micro': 0.996969696969697,\n",
       " 'f1_weighted': 0.9969663228090195,\n",
       " 'precision_macro': 0.9977678571428572,\n",
       " 'precision_micro': 0.996969696969697,\n",
       " 'precision_weighted': 0.9970238095238094,\n",
       " 'recall_macro': 0.9972222222222222,\n",
       " 'recall_micro': 0.996969696969697,\n",
       " 'recall_weighted': 0.996969696969697}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(train_labels, train_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da7ea3ca-a26d-48dc-a135-428b2f62e9a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9891304347826086,\n",
       " 'f1_macro': 0.9926415094339622,\n",
       " 'f1_micro': 0.9891304347826086,\n",
       " 'f1_weighted': 0.9890155865463496,\n",
       " 'precision_macro': 0.9953703703703703,\n",
       " 'precision_micro': 0.9891304347826086,\n",
       " 'precision_weighted': 0.9895330112721418,\n",
       " 'recall_macro': 0.9903846153846154,\n",
       " 'recall_micro': 0.9891304347826086,\n",
       " 'recall_weighted': 0.9891304347826086}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(eval_labels, eval_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e9c898-7352-4c88-ab92-dc97fa512ad9",
   "metadata": {
    "tags": []
   },
   "source": [
    "Well it's look close enough to me.\\\n",
    "It is time to make a submission.\n",
    "\n",
    "## F. Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2f7eaad-d250-40da-8159-6f9a74a83f5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data = HistoDataset(image_test_path, predict=True, train=False)\n",
    "types_test = []\n",
    "for data in test_data:\n",
    "    #inputs = extractor(data['pixel_values'], return_tensors=\"pt\") # ==> wasted days because I forgot to remove that\n",
    "    with torch.no_grad():\n",
    "        x = data['pixel_values'][None, :].to(device)\n",
    "        predicted_label = model(x).logits.argmax(-1).item()\n",
    "        types_test.append(model.config.id2label[predicted_label])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac6af8-05b2-480b-af61-7b44f918e9e7",
   "metadata": {},
   "source": [
    "Reminder of autotrained model results :\\\n",
    "```\n",
    "7    56\n",
    "2    54\n",
    "3    29\n",
    "4    25\n",
    "5    25\n",
    "1    17\n",
    "8     1\n",
    "```\n",
    "If the results are too far from this it is not even worth submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c39a80e4-7cbb-46e9-86cb-811923bad7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7    69\n",
       "2    48\n",
       "3    30\n",
       "4    23\n",
       "1    20\n",
       "5    14\n",
       "6     2\n",
       "8     1\n",
       "Name: type_id, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_test['type_id'] = types_test\n",
    "df_pred = df_test[['id', 'type_id']]\n",
    "display(df_pred['type_id'].value_counts())\n",
    "df_pred.to_csv(submission_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cf694b-2704-4e85-9b11-679e95b4731e",
   "metadata": {},
   "source": [
    "Not bad ! It's close enough to me to what I get with AutoTrain.\\\n",
    "At submission I obtain a F1-score of 0.783742998846 !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6828ba9-035b-4a1b-9a85-70a0264fbf0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Improve\n",
    "\n",
    "## A. Mitigate overfitting\n",
    "\n",
    "Without any data augmentation, this swin classifier overfits quickly on its training data.\\\n",
    "Adding some augmentations allowed me to push far back this overfitting.\n",
    "\n",
    "It also gets me close to AutoTrain results, but still under.\\\n",
    "Nonetheless, the loss gets very low for both train and validation data. It suggests I might have extracted all the information I could on the train data. \\\n",
    "More relevant transformations could dwindle overfitting even more while increasing the volume of data and information at the same time.\n",
    "\n",
    "I did not toy much with the idea of zoom on the image. All input images are taken with the same magnification x40, it might be different on the test set. It could improve the models understanding of ratios of what it sees on the images.\n",
    "\n",
    "Looking at other codes for fine-tuning, I encountered multimodal mixup for fine-tuning [https://github.com/bwconrad/vit-finetune/blob/main/src/mixup.py] which I'm curious about. It supposedly improve performances while being generic.\n",
    "\n",
    "## B. Task related\n",
    "\n",
    "A criticism to this model driven approach is the lack of understanding of what it is I classify.\\\n",
    "Understanding the data might bring better results but even if not easy to add to the Swin transformer. A combination of the Swin transformer others models trained on hand defined features could improve results. Such model combinations were previously popular on challenge platforms such as Kaggle.\n",
    "\n",
    "## C. Time\n",
    "\n",
    "Time being important for the data challenge it is better to be able to iterate quickly.\\\n",
    "Training in this notebook is slow, it takes around 30 minutes to get a idea of how training goes.\\\n",
    "Solutions exists to accelerate training, but could not but be tested on my Shadow machine.\\\n",
    "Solutions include python libraries such as Hugging Face Accelerate, Microsoft DeepSpeed, Amazon Sagemaker, or at least get out of the notebook format to execute on Télécom cluster.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c5da05-f735-44f7-a849-2f7a77566ff1",
   "metadata": {},
   "source": [
    "For quite some time I could not reproduce results from Hugging Face which was problematic. \\\n",
    "My submissions got me bad result on the datachallenge with F1 under 0.40. \\\n",
    "It is only after trying another method involving another model that I realized my mistake : the one contained in the cell of section F when predicting for the submission. \n",
    "```\n",
    "inputs = extractor(data['pixel_values'], return_tensors=\"pt\")\n",
    "```\n",
    "This line of code was coming from the early stage of predicting with the autotrained model.\\\n",
    "Unfortunately it means that I was the image processor on an already processed image.\n",
    "\n",
    "That issue effectively prevented me to go beyond the fine-tuning of swin with the time I could dedicate to the challenge. \n",
    "\n",
    "About the other method I tried, I used Swin to produce an embedding of images with 1028 features and feed it to an XGboost algorithm. A naive version of it gave me poor results while simultanously unlocking my issue mentionned above.\n",
    "\n",
    "Too bad I did not spend time improving over Hugging Face model but the challenge was the opportunity to try state-of-the-art models and benchmark fine-tuning against a variety of methods submitted by other students. It definetly shows impressive performances in little time and computing power if you know how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a9ffa7-2009-476b-8d27-94b140edfb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
